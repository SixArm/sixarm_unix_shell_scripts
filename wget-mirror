#!/bin/sh
set -euf -o pipefail

##
# wget mirror from a remote website URL into a local directory prefix.
#
# We use wget with these settings:
#
#  -m, --mirror : turn on options suitable for mirroring.
#    equivalent to -r -N -l inf --no-remove-listing.
#
#  -c, --continue : resume getting a partially-downloaded file, site, etc.
#
#  -p, --page-requisites : download all files that are necessary to properly display a given HTML page.
#
#  -k, --convert-links : after the download, convert the links in document for local viewing.
#
#  -P, --directory-prefix./LOCAL-DIR : save all the files and directories to the specified directory.
#
#  -np, --no-parent : don't ascend to the parent directory
#
# From http://askubuntu.com/questions/20463/how-can-i-download-an-entire-website
#
# Some flags if you want to roll your own:
#
#    -E, --adjust-extension : save HTML/CSS documents with proper extensions
#
#    --wait=SECONDS : wait wait SECONDS between retrievals
#
#    --random-wait : wait from 0.5*WAIT...1.5*WAIT secs between retrievals
#
# Example of our typical usage to mirror a third-party website directory:
#
#     wget "https://www.example.com/subdirectory/" \
#     --directory-prefix="$HOME/mirrors/www.example.com/" \
#     --mirror --page-requisites --continue --convert-links \
#     --level=2 --wait=2 --random-wait \
#     --domains="www.example.com" \
#     --include-directories="subdirectory,assets"
#
# N.b. another answer suggests these:
#
#  wget
#     --recursive \
#     --no-clobber \
#     --page-requisites \
#     --html-extension \
#     --convert-links \
#     --restrict-file-names=windows \
#     --domains website.org \
#     --no-parent \
#
# Contact: Joel Parker Henderson (joel@joelparkerhenderson.com)
# License: GPL
# Created: 2012-05-06T18:17:43
# Updated: 2024-10-28T17:32:05Z
##

website_url=$1
save_to_directory_prefix=$2
wget "$website_url" --directory-prefix="$save_to_directory_prefix" --mirror --page-requisites --continue --convert-links 
